<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Summary of Byesian Theory | Enjoy Short Life</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="$$X \sim  Beta(\alpha,\beta)$$$$p(x;\alpha,\beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$$Note that if \(\alpha=1 \beta=1\), Beta distribution boil">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary of Byesian Theory">
<meta property="og:url" content="https://sunyasheng.github.io/2018/04/21/Byesian-Summary/index.html">
<meta property="og:site_name" content="Enjoy Short Life">
<meta property="og:description" content="$$X \sim  Beta(\alpha,\beta)$$$$p(x;\alpha,\beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$$Note that if \(\alpha=1 \beta=1\), Beta distribution boils down to uniform distribution. Formally">
<meta property="og:updated_time" content="2018-05-21T03:41:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Summary of Byesian Theory">
<meta name="twitter:description" content="$$X \sim  Beta(\alpha,\beta)$$$$p(x;\alpha,\beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$$Note that if \(\alpha=1 \beta=1\), Beta distribution boils down to uniform distribution. Formally">
  
    <link rel="alternate" href="/atom.xml" title="Enjoy Short Life" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Enjoy Short Life</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://sunyasheng.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Byesian-Summary" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/21/Byesian-Summary/" class="article-date">
  <time datetime="2018-04-21T07:48:11.000Z" itemprop="datePublished">2018-04-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Summary of Byesian Theory
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>$$X \sim  Beta(\alpha,\beta)$$<br>$$p(x;\alpha,\beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$$<br>Note that if \(\alpha=1 \beta=1\), Beta distribution boils down to uniform distribution. Formally,<br>$$Beta(1,1) = U(0,1)$$</p>
<p>$$\theta \sim InvGamma(\alpha, \beta)$$ $$p(\theta) \propto \theta^{-\alpha-1}e^{-\frac{\beta}{\theta}} $$</p>
<p>$$\theta \sim Gamma(\alpha, \beta)$$ $$p(\theta) \propto \theta^{\alpha-1}e^{-\beta \theta} $$</p>
<h3 id="Conjugate-Prior"><a href="#Conjugate-Prior" class="headerlink" title="Conjugate Prior"></a>Conjugate Prior</h3><table>
<thead>
<tr>
<th style="text-align:center">prior distribution</th>
<th style="text-align:center">likelihood</th>
<th style="text-align:center">posterior distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\(Beta(\alpha,\beta)\)</td>
<td style="text-align:center">\(B(n,p)\)</td>
<td style="text-align:center">\(Beta(\alpha+n,\beta+p)\)</td>
</tr>
<tr>
<td style="text-align:center">\(Normal(m_0, \sigma_0)\)</td>
<td style="text-align:center">\(Normal(\theta, \sigma)(\theta unknown)\)</td>
<td style="text-align:center">Normal Distribution(Weigted)</td>
</tr>
<tr>
<td style="text-align:center">\(InvGamma(\alpha, \beta)\)</td>
<td style="text-align:center">\(Normal(\theta, \sigma)(\sigma unknown)\)</td>
<td style="text-align:center">InvGamma( \(\alpha + 0.5, \beta + \frac{(y-m)^2}{2}\) )</td>
</tr>
<tr>
<td style="text-align:center">\(Gamma(\alpha, \beta)\)</td>
<td style="text-align:center">\(Possion(\theta)\)</td>
<td style="text-align:center">\(Gamma(\alpha+y, \beta + 1)\)</td>
</tr>
<tr>
<td style="text-align:center">\(Gamma(\alpha, \beta)\)</td>
<td style="text-align:center">\(Exponential(\theta)\)</td>
<td style="text-align:center">\(Gamma(\alpha+1, \beta + y)\)</td>
</tr>
</tbody>
</table>
<h3 id="Exponential-Family"><a href="#Exponential-Family" class="headerlink" title="Exponential Family"></a>Exponential Family</h3><p>Exponential family is a family of distribution in the form of<br>$$p(y|\theta)=f(y)g(\theta)e^{\phi (\theta) u(y)}$$<br>\(\phi (\theta)\) is the natural parameter of the family.<br>\(u(y)\) is sufficient statistics of \(\theta\), the likelihood function depends on \(y\) only through \(u(y)\).</p>
<p>The conjugate prior of exponential family is $$\theta \sim g(\theta)^a e^{\phi (\theta) b}$$.<br>The \(a\) and \(b\) are parameters of this distribution.<br>In the posterior distribution, the parameters become \(a+1\) and \(b+u(y)\) respectively.</p>
<h3 id="Uninformative-Prior"><a href="#Uninformative-Prior" class="headerlink" title="Uninformative Prior"></a>Uninformative Prior</h3><h4 id="Improper-prior"><a href="#Improper-prior" class="headerlink" title="Improper prior"></a>Improper prior</h4><p>\(\int p(\theta) = \infty \), \(p(\theta) \propto 1\),  \(\theta \in (- \infty, + \infty)\)<br>It may cause the posterior distribution non-integrable</p>
<h4 id="Jeffery’s-invariance-prior"><a href="#Jeffery’s-invariance-prior" class="headerlink" title="Jeffery’s invariance prior"></a>Jeffery’s invariance prior</h4><p>The uninformative prior is supposed to independent of parameterization. It implies that prior is variable-invariant.<br>Define Fisher Information<br>$$J(\theta) = E[(\frac{dlog(p(y|\theta))}{d\theta})^2] = - E[\frac{d^2log(p(y|\theta))}{d\theta^2}]$$<br>Prior is \(p(\theta) = \theta^{-0.5}(1-\theta)^{-0.5}\) when \(p(y|\theta) \propto \theta^y (1-\theta)^{n-y}\) .<br>Prior is $$p(\theta) \propto \frac{1}{\theta} $$ when $$p(y|\theta) \propto \theta^{-0.5}exp(-\frac{(y-m)^2}{2\theta}) $$.<br>Prior is $$p(\theta) \propto 1 $$ when $$p(y|\theta) \propto exp(-\frac{(y-\theta)^2}{2\tau}) $$.</p>
<h3 id="Multi-Parameter-Model"><a href="#Multi-Parameter-Model" class="headerlink" title="Multi Parameter Model"></a>Multi Parameter Model</h3><table>
<thead>
<tr>
<th style="text-align:left">prior</th>
<th style="text-align:center">model</th>
<th style="text-align:right">posterior</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">\(p(\theta_1, \theta_2)\)</td>
<td style="text-align:center">\(p(y \mid \theta_1, \theta_2)\)</td>
<td style="text-align:right">\(p(\theta_1, \theta_2 \mid y)\)</td>
</tr>
</tbody>
</table>
<p>If we are only interested in \(\theta_1\), we call \(\theta_2\) the nuisance parameter. The existance of nuisance parameter adds uncertainty to this model. Formally,<br>$$p(\theta_1 \mid y) = \int p(\theta_1, \theta_2 \mid y) d\theta_2 = \int p(\theta_1 \mid \theta_2, y)p(\theta_2 \mid y) d\theta_2$$<br>Final \(p(\theta_1 \mid y)\) is mixture of lots of \(p(\theta_1 \mid \theta_2, y)\) parametered by \(\theta_2\).</p>
<h3 id="Uninformative-Prior-in-Multi-Parameter-Model"><a href="#Uninformative-Prior-in-Multi-Parameter-Model" class="headerlink" title="Uninformative Prior in Multi Parameter Model"></a>Uninformative Prior in Multi Parameter Model</h3><h4 id="Jeffery’s-Prior"><a href="#Jeffery’s-Prior" class="headerlink" title="Jeffery’s Prior"></a>Jeffery’s Prior</h4><p>Now Suppose that our model is Normal Distribution. \(p(y \mid \theta) \sim N(\mu, \tau^2)\)<br>Based on the Jeffery’s Prior, the prior indepence between multi-varibale can be proved.<br>$$p(\mu, \tau^2) = p(\mu)p(\tau^2) = 1 * \frac{1}{\tau^2}$$<br>The posterior become $$p(\mu, \tau^2 \mid y) \propto \tau^{-n-2} exp[-\frac{S^2+n(\hat y - \mu)^2}{2\tau^2}]$$ $$S^2 = (y_i - \hat y)^2$$<br>we got</p>
<table>
<thead>
<tr>
<th style="text-align:left">\(p(\mu \mid \tau^2, y)\)</th>
<th style="text-align:center">\(p(\tau^2 \mid y)\)</th>
<th style="text-align:right">\(p(\mu \mid y)\)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">\(N(\hat y, \frac{n}{\tau^2})\)</td>
<td style="text-align:center">\(invKappa(n-1, S^2)\)</td>
<td style="text-align:right">\(t_\{n-1\} (\hat y, \frac{S^2}{n})\)</td>
</tr>
</tbody>
</table>
<h3 id="Theoretical-Assurance-of-Baysian-Model-Corretness"><a href="#Theoretical-Assurance-of-Baysian-Model-Corretness" class="headerlink" title="Theoretical Assurance of Baysian Model Corretness"></a>Theoretical Assurance of Baysian Model Corretness</h3><p>With infinite nuumber of data, the probability of this distribution \(\theta\) will collapse to the true \(\theta^\star\).</p>
<h4 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h4><ol>
<li>\((y_1, y_2, …, y_n) \sim p(y \mid \theta^\star)\) iid</li>
<li>There exist \(\theta_0\) such that \(D_KL(p(y \mid \theta^\star), p(y \mid \theta))\) is minimized. \(p(y \mid \theta)\) is our proposed model.</li>
</ol>
<h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>Convergence of the posterior for discrete parameter space, if our parameter space H is finite and \(p(\theta = \theta_0) &gt; 0\), \(p(\theta = \theta_0 \mid y) \to 1, n \to \infty\)<br>Proof:<br>$$log(\frac{p(\theta \mid y)}{p(\theta_0 \mid y)}) = log(\frac{p(\theta)p(y \mid \theta)}{p(\theta_0)p(y \mid \theta_0)})$$ $$ = log \frac{p(\theta)}{\theta_0} + log \frac{p(y \mid \theta)}{p(y \mid \theta_0)} $$ $$ = log \frac{p(\theta)}{\theta_0} + \sum_i^n log \frac{p(y_i \mid \theta)}{p(y_i \mid \theta_0)}$$</p>
<p>$$ E(log \frac{p(y_i \mid \theta)}{p(y_i \mid \theta_0)}) = E_f(log(\frac{f}{log(p(y_i \mid \theta_0))})) - E_f(log(\frac{f}{log(p(y_i \mid \theta))})) &lt; 0$$<br>\(f = p(y \mid \theta^\star)\) is the true distribution.<br>Therefore, right side of this equation converges toward inifinity when n is infinite. Plus, \(p(\theta_0 \mid y)\) is not zero. We obtain \(p(\theta \mid y) \to 0, \forall \theta \ne \theta_0 \Rightarrow p(\theta_0 \mid y) = 1 \)</p>
<p>Specifically, \(p(\theta \mid y) \to N(\theta_0, \frac{1}{n J(\theta_0)})\) under some regularity conditinos.</p>
<h3 id="Decision-of-hyerparameter-Prior-Distribution"><a href="#Decision-of-hyerparameter-Prior-Distribution" class="headerlink" title="Decision of hyerparameter(Prior Distribution)"></a>Decision of hyerparameter(Prior Distribution)</h3><h4 id="Hierachical-Bayes-Full-Bayes"><a href="#Hierachical-Bayes-Full-Bayes" class="headerlink" title="Hierachical Bayes(Full Bayes)"></a>Hierachical Bayes(Full Bayes)</h4><p>Make the prior hyerparameter also probabilistic (recursive).<br>$$p(\theta, \alpha \mid y) \propto p(\theta, \alpha) p(y \mid \theta, \alpha) \propto p(y \mid \theta) p(\theta \mid \alpha) p(\alpha)$$ $$p(\theta \mid y) = \int p(\theta, \alpha \mid y) d \alpha$$</p>
<h4 id="Empirical-Bayes"><a href="#Empirical-Bayes" class="headerlink" title="Empirical Bayes"></a>Empirical Bayes</h4><p>$$\hat \alpha = argmax p(y \mid \alpha) = \int p(y \mid \theta) p(\theta \mid \alpha) d\theta$$ $$p(\theta \mid y) = \int p(\theta \mid \alpha, y) p(\alpha \mid y) d\alpha  = \sum_{i=1}^n w_i p(\theta \mid \alpha_i, y) $$<br>Empirical distribution chooses largest probabilistic distribution \( \hat \alpha = \alpha_i \), but full bayes mixes these distributions.</p>
<h5 id="stein’s-example"><a href="#stein’s-example" class="headerlink" title="stein’s example"></a>stein’s example</h5><p>$$p(y\mid \theta) \sim N(0,\sigma^2 I_n)$$ $$p(\theta) \sim N(0,\tau^2 I_n)$$ $$ p(\theta \mid y) \sim N((1 - \frac{\sigma^2}{\sigma^2 + \tau^2})y, \frac{\sigma^2 \tau^2}{\sigma^2+\tau^2} I_n) $$<br>Denote point estimation \(\hat A = \frac{1}{\sigma^2 + \tau^2}\). It’s unbiased estimation is \(\hat A = \frac{n-2}{y^Ty}\). \(\hat \theta_{EB} = (1-\sigma^2 \hat A)y\). This empirical estimation is called James-Stein estimation, denoted by \(\theta_{JS}\).<br>In this case, the components of \(\theta\) is not independent because the \(\hat A\) depends on all the data and affects the \(\hat \theta_{EB}\)<br>Back to statistics, \(\theta\) is supposed to be estimated to \(y\). Formally,<br>$$\hat \theta = y$$<br>Here comes the question. What kind of estimation is supposed to be an excellent estimation? Smaller \(R(\theta, \hat \theta)\) is, better this estimation is. (Usually, the estimation can be divided into two parts including variance and bias. For unbiased estimation, bias equals zero).<br>$$R(\theta, \hat \theta_{JS}) = E[(\theta - \hat \theta_{JS})^2] $$ $$= E[\theta - y + y \frac{n-2}{\parallel y \parallel ^2}] = E[(\theta - y)^2] + 2(n-2) E[\frac{(\theta -y)^Ty}{\parallel y \parallel ^2}] + (n-2)^2 E[\frac{1}{\parallel y \parallel^2}]$$<br>Introduce stein’s Lemma:<br>$$E[(\theta_i - y_i)h(y)] = -E[\frac{\partial h(y_i)}{\partial y_i} ]$$<br>Let<br>$$h(y)= \frac{y_i}{\parallel y \parallel_2^2}$$<br>We obtain<br>$$E[\frac{(\theta - y)^T y}{\parallel y \parallel ^2}] = \sum_i^n E[\frac{(\theta_i -y_i)y_i}{\parallel y \parallel ^2}] $$ $$= -\sum_i^n E[\frac{1}{\parallel y \parallel^2} - \frac{2y_i^2}{\parallel y \parallel^2}] = - (n-2) E[\frac{1}{\parallel y \parallel^2}]$$<br>To sum up,<br>$$ R(\theta, \hat \theta_{JS}) = E[(\theta - y)^2] - (n-2)^2 E[\frac{1}{\parallel y \parallel^2}] &lt;  E[(\theta - y)^2]$$<br>It implies that the JS estimater is a better estimation. But it seems a little wired that we could obtain a better result by just adding more components to \(y\) (boost \(n\)). That’s why this theory is broadly criticized when it is first published. </p>
<h3 id="MC-Markov-Chain"><a href="#MC-Markov-Chain" class="headerlink" title="MC(Markov Chain)"></a>MC(Markov Chain)</h3><p>A Stocastical Process is called a Markov Process when it satisfies those two conditions.</p>
<ol>
<li>\(p(x^{i} \mid x^{i-1}, x^{i-2}, … , x^{1}) = T(x_{i} \mid x_{i-1})\)</li>
<li>\(T \triangleq T(x^{i}\mid T(x^{i-1}))\) invariant for all i.</li>
</ol>
<p>A Markov Process will converges to an invariant distribution when it satisfies two conditions.</p>
<ol>
<li>Irreducibility</li>
<li>Aperiodicity</li>
</ol>
<p>A sufficient but not necessary condition for a chain to converge to an invariant distribution is the detailed balance condition.<br>$$P(x^i)T(x^{i-1}\mid x^{i}) = P(x^{i-1})T(x^i \mid x^{i-1})$$<br>Metropolis-Hasting algorithm is just sampling and rejecting to force the sample to walk towards high probability place. This process satisfies the detailed balance condition such that it finally converges to the original probability distribution.</p>
<h4 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h4><p>The proposed distribution is defined as<br>$$ q(x^\star \mid x^i) = p(x_j^{\star} \mid x_{-j}^{i}) 1[x_{-j}^{\star} = x_{-j}^i]$$</p>
<h3 id="MCMC-Markov-Chain-Monte-Carlo"><a href="#MCMC-Markov-Chain-Monte-Carlo" class="headerlink" title="MCMC(Markov Chain Monte-Carlo)"></a>MCMC(Markov Chain Monte-Carlo)</h3><h4 id="Adaptive-MCMC"><a href="#Adaptive-MCMC" class="headerlink" title="Adaptive MCMC"></a>Adaptive MCMC</h4><h5 id="Independence-Sampler"><a href="#Independence-Sampler" class="headerlink" title="Independence Sampler"></a>Independence Sampler</h5><p>Intuitively, we accept our proposed sample with a higher probability when the proposed sampler is more similar with the original probability distribution. If the proposed probability distribution is Gaussian, it requires the mean and coveriance is sufficient to describe the original probability distribution. Therefore, algorithm can be stated as first sample the data following the traditional routine to estimate mean and coveriance and than make use of this estimated parameter to do the independence sampler. </p>
<h5 id="Adaptive-Metropolis-Algorithm"><a href="#Adaptive-Metropolis-Algorithm" class="headerlink" title="Adaptive Metropolis Algorithm"></a>Adaptive Metropolis Algorithm</h5><p>Recall that in Metropolis Algorithm, the proposed sample is random walk surrounding \(x_n\). Formally, \(x^\star = x_n + \epsilon\), \(\epsilon \sim N(0, \sigma^2 I_n)\). When the dimension of the variable is dependent and not isotropic, the random walk behave poorly. A better coveriance \(C_t\) is required to describe the original distribution. So the proposed distribution is defined as<br>$$ p(x^\star \mid x_n) = N(x_n, \sigma^2 C_t)$$ </p>
<p>The \(C_t\) is updated at every time t following<br>$$Cov(X_0, X_1, … , X_t) = \frac{\sum_{i=1}^{t-1}(x_ix_i^{t} - ((t-1)+1)\bar x_{t-1}\bar x_{t-1}^T)}{t}$$.<br>$$\bar x_{t-1} = \frac{\sum_{i=0}^{t-1} x_i}{t}$$. </p>
<p>Or we could compute \(C_t\) recursively, $$C_{t+1} = \frac{(t-1)C_t}{t} + \frac{t\bar x_{t-1} \bar x_{t-1}^T - (t+1)\bar x_t \bar x_t ^T + x_t x_t^T}{t}$$ </p>
<p>Although the Markov Property can not be guranteed here, it has been proven that this algorithm will finally converges to the original probability without using the detailed balance condition.</p>
<h3 id="Metrics-to-Evaluate-the-MCMC-Algorithm"><a href="#Metrics-to-Evaluate-the-MCMC-Algorithm" class="headerlink" title="Metrics to Evaluate the MCMC Algorithm"></a>Metrics to Evaluate the MCMC Algorithm</h3><p>The data generated by MCMC algorithm is related violating independent sampling rule we expected. But how many indepedent samples are there or what is the effective smaple size in all the samples. The ESS(effective sample size) is empirically computed following<br>$$ESS = \frac{n}{\sum_{\tau=1}^n R(\tau)}$$</p>
<p>The self-correlation is used to evaluate the MCMC.<br>$$ R(s,t) = \frac{E(X_t - \mu_t)(X_s - \mu_s)}{\sigma_s \sigma_t}$$.<br>In Markov Chain, \(\mu = const, \sigma = const\). So the \(R(s,t) = R(t-s) = R(\tau)\). In matlab, \(autocorr(x, size)\) is defined to compute \(R(\tau) \sim \tau\) and \(mhsample()\) to do Metropolis Sampling. Usually, \(R(\tau)\) vanishes slowly when the accept probability is too low or too high. Empirically, the \(R(\tau)\) vanishes quickly when the accept probability belongs to 0.3 to 0.7 under an appropriate step size.</p>
<h3 id="Decision-Theory"><a href="#Decision-Theory" class="headerlink" title="Decision Theory"></a>Decision Theory</h3><p>Parameter Estimatation is essentially a decision theroy problem. Find a \(\theta^{\star} \) based on<br>$$ p(\theta \mid y) $$ through Loss Expectation Minimization or other loss information.</p>
<p>In a decision problem, many information are unknown called state of nature( unknown parameter ): \( \theta \in \Theta \). History data \( x \in X \) is provided to give us information. The possible action is denoted by \( a \in A \). Specifically, the action \( a \) is an estimation of \( \theta \) in parameter estimation.</p>
<p>There key information:</p>
<ol>
<li>Loss function: \( L(\theta, a) \) represents the loss caused by action \(a\) when parameter is  \( \theta \).</li>
<li>Prior distribution of \( \theta \): \( \pi(\theta) \).</li>
<li>Likelihood function: \( f(X \mid \theta) \)</li>
</ol>
<p>Example 1: </p>
<ol>
<li>\( \theta \in [0, 1] \): proportion of market.</li>
<li>\( a \in [0, 1]\): estimation.</li>
<li>\( x \): data from market investigation.</li>
</ol>
<p>Loss:<br>$$<br>L(\theta, a) =<br>    \begin{cases}<br>        \theta - a &amp; \theta \geq a \\<br>        2(a - \theta) &amp; a \geq \theta<br>    \end{cases}<br>$$</p>
<p>Model:<br>$$f(x \mid \theta) = C_n^x \theta ^x (1 - \theta)^(n-x)$$</p>
<p>Prior:<br>$$ U[0.1, 0.2] $$ </p>
<p>Example 2: </p>
<ol>
<li>\( \theta \in [0,1]\): defective rate.</li>
<li>\((x, n)\): defective samples in all samples.</li>
<li>\(a \in (A, R)\): A is acception and R is Rejection.</li>
</ol>
<p>Loss:<br>$$ L(\theta, A) = 10 \theta $$ $$ L(\theta, R) = const $$</p>
<p>Model:<br>$$f(x \mid \theta) = C_n^x \theta ^x (1 - \theta)^{(n-x)}$$</p>
<p>Prior:<br>$$ U[0, 1] $$ or $$ Beta(0.05, 1) $$</p>
<p>Decision Rule(Frequentist):<br>\( a = \delta(x) \): \( X \to A \)</p>
<p>Risk function:<br>$$ R(\theta, \delta) = E_{\theta}^{x}[ L(\theta , \delta(x))] = \int L(\theta, \delta(x)) f(x \mid \theta) dx $$</p>
<ol>
<li><p>The Bayes risk principle:<br>A decision rule \(\delta_1 \) is prefered to rule \( \delta_2 \) if<br>$$ r(\pi , \delta_1) &lt; r(\pi, \delta_2)$$ where<br>$$ r(\pi, \delta) = E^{\pi}(R(\theta, \delta)) = \int R(\theta, \delta) \pi(\theta) d\theta $$</p>
</li>
<li><p>The minmax principle:<br>A decision rule \( \delta_1 \) is prefered to rule \( \delta_2 \) if<br>$$ sup_\theta  R(\theta, \delta_1) &lt; sup_\theta R(\theta, \delta_2)$$ </p>
</li>
</ol>
<p>A decision rule \(p_1\) is said to be R-Better that \(p_2\) if \(R(\theta, p_1) \leq R(\theta, p_2)\) for all \(\theta \in \Theta\), with some strict inequality for some \( \theta \). A decision rule is admissable if there exists no R-Better decision rule. </p>
<p>Expected Loss:<br>$$ E_{p(\theta)}(L(\theta, a)) = \int L(\theta, a) p(\theta) d\theta$$<br>If \( p(\theta) = \pi(\theta) \), we call it prior expected loss. Contrastly, \( p(\theta) = \pi(\theta \mid x) \) is posterior expected loss.</p>
<p>The Bayesian(Posterior) expected loss:<br>\( \int L(\theta, a) \pi(\theta \mid x) d\theta  \)<br>Choose an action \(a \in \triangle \) by minimize expected loss. Such an action is called a Bayes action. </p>
<h3 id="Acknowlegement"><a href="#Acknowlegement" class="headerlink" title="Acknowlegement"></a>Acknowlegement</h3><p>Thank Prof. Li for his amazing lecture. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://sunyasheng.github.io/2018/04/21/Byesian-Summary/" data-id="cjhfpevve0004vys6h31fo17w" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/03/18/Torch-Tutorial/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Torch_Tutorial</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/04/21/Byesian-Summary/">Summary of Byesian Theory</a>
          </li>
        
          <li>
            <a href="/2018/03/18/Torch-Tutorial/">Torch_Tutorial</a>
          </li>
        
          <li>
            <a href="/2018/03/04/GAN-Workflow/">GAN Workflow</a>
          </li>
        
          <li>
            <a href="/2018/02/04/Introduction-of-Pascal-VOC-Dataset/">Introduction of Pascal VOC Dataset</a>
          </li>
        
          <li>
            <a href="/2018/02/04/Frequently-Used-Script/">Frequently Used Script</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  
	<!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": { 
        preferredFont: "TeX", 
        availableFonts: ["STIX","TeX"], 
        linebreaks: { automatic:true }, 
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) 
    },
    tex2jax: { 
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ], 
        processEscapes: true, 
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {  
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, 
        Macros: { href: "{}" } 
    },
    messageStyle: "none"
    }); 
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Yasheng Sun<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>